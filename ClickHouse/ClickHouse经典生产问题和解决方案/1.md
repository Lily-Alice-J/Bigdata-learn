##ClickHouse报错总结

1.1 问题再现

    Memory limit (for query) exceeded:would use 9.37 GiB (attempt to allocate chunk of 301989888 bytes), maximum: 9.31 GiB

1.2 分析问题

    默认情况下，ClickHouse会限制了SQL的查询内存使用的上线，当内存使用量大于该值的时候，查询被强制KILL。
    
1.3 解决问题

    对于常规的如下简单的SQL, 查询的空间复杂度为O(1) 。
    select count(1) from table where column1=xxx;
    对于group by, order by , count distinct这样的复杂的SQL，查询的空间复杂度就不是O(1)了，需要使用大量的内存。
        ·如果是group by内存不够，推荐配置上max_bytes_before_external_group_by参数，当使用内存到达该阈值，进行磁盘group by。推荐配置为max_memory_usage的一半
        ·如果是order by内存不够，推荐配置上max_bytes_before_external_sort参数，当使用内存到达该阈值，进行磁盘order by。
        ·如果是count distinct内存不够，推荐使用一些预估函数(如果业务场景允许)，这样不仅可以减少内存的使用同时还会提示查询速度。
        
2.1 问题再现

    Progress: 157.94 million rows, 6.91 GB (92.63 thousand rows/s., 4.05 MB/s.) Received exception from server (version 19.4.0):
    Code: 319. DB::Exception: Received from 10.0.0.50:9000. DB::Exception: Unknown status, client must retry. Reason: Connection loss.
    Progress: 94.47 million rows, 4.18 GB (95.07 thousand rows/s., 4.20 MB/s.) Received exception from server (version 19.4.0):
    Code: 999. DB::Exception: Received from 10.0.0.50:9000. DB::Exception: Cannot allocate block number in ZooKeeper: Coordination::Exception: Connection loss.
    lineorder_flat_all.Distributed.DirectoryMonitor: Code: 225, e.displayText() = DB::Exception: Received from ambari02:9000, 10.0.0.52. DB::Exception: ZooKeeper session has been expired.. Stack trace:

2.2 分析问题

    根据报错信息可知，是因为与Zookeeper的连接丢失导致不能分配块号等问题。因为clickhouse对zookeeper的依赖非常的重，表的元数据信息，每个数据块的信息，每次插入的时候，数据同步的时候，都需要和zookeeper进行交互。
    zookeerper 服务在同步日志过程中，会导致ZK无法响应外部请求，进而引发session过期等问题。
    
2.3 解决问题

    ·加大zookeeper会话最大超时时间，在zoo.cfg 中修改MaxSessionTimeout=120000，修改后重启zookeeper。注意：zookeeper的超时时间不要设置太大，在服务挂掉的情况下，会反映很慢。zookeeper的snapshot文件存储盘不低于1T，注意清理策略
    ·在zookeeper中将dataLogDir存放目录应该与dataDir分开，可单独采用一套存储设备来存放ZK日志。
    ·在ZOO.CFG中增加：forceSync=no。默认是开启的，为避免同步延迟问题，ZK接收到数据后会立刻去将当前状态信息同步到磁盘日志文件中，同步完成后才会应答。
     将此项关闭后，客户端连接可以得到快速响应。关闭forceSync选项后，会存在潜在风险，虽然依旧会刷磁盘（log.flush()首先被执行），但因为操作系统为提高写磁盘效率，会先写缓存，当机器异常后，可能导致一些zk状态信息没有同步到磁盘，从而带来ZK前后信息不一样问题。
    ·建表的时候添加use_minimalistic_part_header_in_zookeeper参数，对元数据进行压缩存储，但是修改完了以后无法再回滚的。
    
3.1 问题再现

    <Error> lineorder_flat_all.Distributed.DirectoryMonitor: Code: 242, e.displayText() = DB::Exception: Received from ambari04:9000, 10.0.0.54. DB::Exception: Table is in readonly mode. Stack trace:

3.2 分析问题

    因为zookeeper集群出问题(例如zk服务挂了)导致的压力太大，表处于“read only mode”模式，导致插入失败。
    
3.3 解决问题

    ·做好zookeeper集群和clickhouse集群的规划，可以多套zookeeper集群服务一套clickhouse集群。
    ·zookeeper机器的snapshot文件和log文件最好分盘存储(推荐SSD)提高ZK的响应；
    ·在zoo.cfg中增加forceSync=no。解释同上2.3。
    
4.1 问题再现

    Cannot create table from metadata file /var/lib/clickhouse/metadata/xx/xxx.sql, error: Coordination::Exception: Can’t get data for node /clickhouse/tables/xx/cluster_xxx-01/xxxx/metadata: node doesn’t exist (No node), stack trace:

4.2 分析问题

     因为zookeeper数据丢失，从而使clickhouse数据库无法启动。
     
4.3 解决问题

    1.将/var/lib/clickhouse/metadata/ 下的SQL与/var/lib/clickhouse/data/ 下的数据备份之后删除；
    2.启动数据库；
    3.创建与原来表数据结构的MergeTree表；
    4.将之前分布式表的数据文件夹复制到新表的数据目录中；
    5.重启数据库；
    6.重新创建原结构本地表；
    7.重新创建原结构分布式表；
    8.insert into [分布式表] select * from [MergeTree表]。