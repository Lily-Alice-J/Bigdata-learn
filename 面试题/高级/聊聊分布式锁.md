## 聊聊分布式锁

### 为什么我们需要一把分布式锁？
```text
    单机体系结构的应用可以通过直接使用同步或ReentrantLock解决多线程资源竞争问题。如果公司业务发展迅速，可以通过部署多个服务节点来提高系统的并行处理能力。 因为本地锁定仅限于当前应用的线程。 在高并发方案中，分布式锁定很有帮助，因为群集中一个APP应用程序的本地锁定不排斥其他APP应用程序的资源访问，而是导致数据不一致。
    在分布式系统中，同一时间只允许一个线程/进程对共享资源进行操作。例如：秒杀、积分扣减、抢红包、定时任务执行等等。
    
    1.为了效率(efficiency)，协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的 email（当然这取决于业务应用的容忍度）。
    2.为了正确性(correctness)。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，订单重复，超卖或者其它严重的问题。
```

### 分布式锁的三个属性
```text
-- 互斥（Mutual Exclusion），这是锁最基本的功能，同一时刻只能有一个客户端持有锁；
-- 避免死锁（Dead lock free），如果某个客户端获得锁之后花了太长时间处理，或者客户端发生了故障，锁无法释放会导致整个处理流程无法进行下去，所以要避免死锁。最常见的是通过设置一个 TTL(Time To Live，存活时间) 来避免死锁。
-- 容错（Fault tolerance），为避免单点故障，锁服务需要具有一定容错性。大体有两种容错方式，一种是锁服务本身是一个集群，能够自动故障切换(ZooKeeper、etcd)；另一种是客户端向多个独立的锁服务发起请求，其中某个锁服务故障时仍然可以从其他锁服务读取到锁信息(Redlock)，代价是一个客户端要获取多把锁，并且要求每台机器的时钟都是一样的，否则 TTL 会不一致，可能有的机器会提前释放锁，有的机器会太晚释放锁，导致出现问题。
```

### 常见的分布式锁实现方案
```text
1.Redis
2.MySQL
3.ZooKeeper
```

#### 基于 Redis 的分布式锁

错误的加锁：非原子操作
```java
public void lock_error1(String lockKey, String requestId, int expireTime) {
RedisCache cache = redisFactory.getRedisCacheInstance(name);
Long result = cache.setnx(lockKey, requestId);
if (result == 1) {
// 加锁和设置超时两个操作是分开的，并非原子操作。假设加锁成功，但是设置锁超时失败，那么该 lockKey 永不失效 将发生死锁。
cache.expireSeconds(lockKey, expireTime);
}
}
```

问题：为什么这个锁必须要设置一个过期时间？
```text
当一个客户端获取锁成功之后，假如它崩溃了，或者它忘记释放锁，或者由于发生了网络分割（network partition）导致它再也无法和 Redis 节点通信了，那么它就会一直持有这个锁，而其它客户端永远无法获得锁了。
```

正确的加锁姿势
```shell
SET lockKey requestId NX PX 30000

lockKey 是加锁的锁名；
requestId 是由客户端生成的一个全局唯一随机字符串，它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的；这个可避免释放别人的锁；
NX 表示只有当 lockKey 对应的 key 值不存在的时候才能 SET 成功。这保证了只有第一个请求的客户端才能获得锁，而其它客户端在锁被释放之前都无法获得锁；
PX 30000 设置过期时间，表示这个锁有一个 30 秒的自动过期时间。当然，这里 30 秒只是一个例子，客户端可以选择合适的过期时间。
```
```java
String result = jedis.set(lockKey, requestId, "NX", "PX", expireTime)
```
```text
1.加锁；
2.业务操作；
3.主动释放锁；
4.如果主动释放锁失败了，则达到超时时间，Redis 自动释放锁。Java 代码里在 finally 中释放锁，即无论代码执行成功或者失败，都要释放锁。
```
释放锁的问题：非原子操作
```java
// 获取该锁的requestId值 && 判断是否是自己加的锁 
// 有可能在判断是否是自己加的锁这步阻塞 导致过期时间到删除锁，另一线程得到，而后续上衣线程从阻塞中恢复过来，执行 DEL 操纵，释放掉了这一线程持有的锁
if (jedis.get(lockKey).equals(requestId)) {
    // 删除锁
    jedis.del(lockKey);
    return true;
}
return false;
```
正确的释放锁姿势
```text
问题的根源：锁的判断在客户端，但是锁的删除却在服务端！
正确的释放锁姿势——锁的判断和删除都在服务端（Redis），使用 lua 脚本保证原子性：
```
```java
public class JedisCommandLock {

    private static final Long RELEASE_SUCCESS = 1L;

    /**
     * 功能：释放分布式锁
     * @param jedis        Redis客户端
     * @param lockKey      锁
     * @param requestId    请求标识
     * @return             是否释放成功
     */
    public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) {
        // Lua 脚本，将判断锁和释放锁变为一步操作
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
        // eval()方法，是将Lua代码交给Redis服务端执行
        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));
        return RELEASE_SUCCESS.equals(result);
    }
}
```

锁超时问题

如果客户端请求锁成功了，但是由于业务处理、GC、操作系统等原因导致它处理时间过长，超过了锁的时间，这时候 Redis 会自动释放锁，这种情况可能导致问题。
```text
如何解决这种问题？---- 续期，Java 里我们可以使用 TimerTask 类来实现自动续期的功能。
这个机制在 Redisson 框架中已经实现，而且还有一个比较霸气的名字 watchdog（看门狗）：加锁时没有指定加锁时间时会启用 watchdog 机制，默认加锁 30 秒，每 10 秒钟检查一次，如果存在就重新设置 过期时间为 30 秒（即 30 秒之后它就不再续期了）
```

##### Redis 主从架构数据同步复制问题

丢失数据场景：当网络发生脑裂（split-brain）或者 partitioned cluster 集群分裂为多数派与少数派，如果数据继续写入少数派的 Master，则当 Cluster 感知，并停止少数派 Master，或者重新选主时，则面临丢失刚才已写入少数派的数据
```text
解决办法1： WAIT 命令
WAIT 命令作用：WAIT 命令阻塞当前客户端，直到所有先前的写入命令成功传输，并且由至少指定数量的副本（slave）确认。在主从、sentinel 和 Redis 群集故障转移中， WAIT 能够增强（仅仅是增强，但不是保证）数据的安全性，但不能保证 Redis 的强一致性。

解决办法2：Redlock 算法
在 Redlock 之前，很多人对于分布式锁的实现都是基于单个 Redis 节点的。而 Redlock 是基于多个 Redis 节点（都是 Master）的一种实现。前面基于单 Redis 节点的算法是 Redlock 的基础。

1.获取当前时间 T1（毫秒数）。
2.使用相同的 key、value 按顺序依次向 N 个 Redis 节点执行获取锁的操作。这个获取操作跟前面基于单 Redis 节点的获取锁的过程相同，包含随机字符串 my_random_value，也包含过期时间(比如 PX 30000，即锁的有效时间)。为了保证在某个 Redis 节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个 Redis 节点获取锁失败以后，应该立即尝试下一个 Redis 节点。
3.获取当前时间 T2 减去步骤 1 中的 T1，计算获取锁消耗了多长时间（T3= T2-T1），计算方法是用当前时间减去第 1 步记录的时间。如果客户端从大多数 Redis 节点（大于等于 N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
4.如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第 3 步计算出来的获取锁消耗的时间。
5.如果最终获取锁失败了（可能由于获取到锁的 Redis 节点个数少于 N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有 Redis 节点发起释放锁的操作（即前面介绍的 Redis Lua 脚本）。
tip: 高并发场景下，当多个加锁线程并发抢锁时，可能导致脑裂，最终造成任何一个线程都无法抢到锁的情况。所以当一个加锁线程无法获得锁的时候，应该在一个随机延时后再一次尝试获得锁。加锁线程从多数 Redis 实例中获得锁越快，出现脑裂的窗口越小（重试的次数也越少）。所以理想情况下，加锁线程应该多路复用地同时向 N 个实例发送加锁命令。

崩溃恢复（AOF 持久化）对 Redlock 算法影响
假设 Rodlock 算法中的 Redis 发生了崩溃-恢复，那么锁的安全性将无法保证。假设加锁线程在 5 个实例中对其中 3 个加锁成功，获得了这把分布式锁，这个时候 3 个实例中有一个实例被重启了。重启后的实例将丢失其中的锁信息，这个时候另一个加锁线程可以对这个实例加锁成功，此时两个线程同时持有分布式锁。锁的安全性被破坏。
如果我们配置了 AOF 持久化，只能减少它发生的概率而无法保证锁的绝对安全。断电的场景下，如果 Redis 被配置了默认每秒同步数据到硬盘，重启之后 lockKey 可能会丢失，理论上，如果我们想要保证任何实例重启的情况下锁都是安全的，需要在持久化配置中设置fsync=always，但此时 Redis 的性能将大大打折扣。

总结：
    1.持久化配置中设置fsync=always，性能大大降低
    2.恰当的运维，把崩溃节点进行延迟重启，超过崩溃前所有锁的 TTL 时间之后才加入 Redlock 节点组
    3.RedLock 算法数建立在了 Time 是可信的模型上的一种分布式锁，所以时间被破坏的情况下它无法实现锁的绝对安全；
    4.RedLock 算法实现比较复杂，并且性能比较差；
    5.RedLock 需要恰当的运维保障它的正确性，故障-崩溃之后需要一套延迟重启的机制
```

#### 基于 MySQL 的分布式锁（ShedLock）

使用 ShedLock 需要在 MySQL 数据库创建一张加锁用的表：
```roomsql
CREATE TABLE shedlock
(
    name VARCHAR(64),
    lock_until TIMESTAMP(3) NULL,
    locked_at TIMESTAMP(3) NULL,
    locked_by VARCHAR(255),
    PRIMARY KEY (name)
)
```
加锁：通过插入同一个 name(primary key)，或者更新同一个 name 来抢，对应的 intsert、update 的 SQL 为：
```roomsql
INSERT INTO shedlock
(name, lock_until, locked_at, locked_by)
VALUES
(锁名字,  当前时间+最多锁多久,  当前时间, 主机名);


UPDATE shedlock
SET lock_until = 当前时间+最多锁多久,
locked_at = 当前时间,
locked_by = 主机名 WHERE name = 锁名字 AND lock_until <= 当前时间
```
释放锁：通过设置 lock_until 来实现释放，再次抢锁的时候需要通过 lock_util 来判断锁失效了没。对应的 SQL 为：
```roomsql
UPDATE shedlock
SET lock_until = lockTime WHERE name = 锁名字
```

#### 基于 ZooKeeper 的分布式锁

```text
临时顺序节点（EPHEMERAL_SEQUENTIAL）【使用该类型节点实现分布式锁】
在创建节点时，ZooKeeper 根据创建的时间顺序给该节点名称进行编号；当创建节点的客户端与 ZooKeeper 断开连接后，临时节点会被删除。

ZooKeeper 的 watch 机制
ZooKeeper 集群和客户端通过长连接维护一个 session，当客户端试图创建/lock 节点的时候，发现它已经存在了，这时候创建失败，但客户端不一定就此返回获取锁失败。客户端可以进入一种等待状态，等待当/lock 节点被删除的时候，ZooKeeper 通过 watch 机制通知它，这样它就可以继续完成创建操作（获取锁）。这可以让分布式锁在客户端用起来就像一个本地的锁一样：加锁失败就阻塞住，直到获取到锁为止。这样的特性 Redis 的 Redlock 就无法实现。

加锁&释放锁
    1.客户端尝试创建一个 znode 节点，比如/lock。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode 已存在），获取锁失败。
    2.持有锁的客户端访问共享资源完成后，将 znode 删掉，这样其它客户端接下来就能来获取锁了。（客户端删除锁）
    3.znode 应该被创建成 EPHEMERAL_SEQUENTIAL 的。这是 znode 的一个特性，它保证如果创建 znode 的那个客户端崩溃了，那么相应的 znode 会被自动删除。这保证了锁一定会被释放（ZooKeeper 服务器自己删除锁）。另外保证了公平性，后面创建的节点会加在节点链最后的位置，等待锁的客户端会按照先来先得的顺序获取到锁。
    
时钟变迁问题
ZooKeeper 不依赖全局时间，它使用 zab 协议实现分布式共识算法，不存在该问题。

超时导致锁失效问题
ZooKeeper 不依赖有效时间，它依靠心跳维持锁的占用状态，不存在该问题。

基于 ZooKeeper 的分布式锁存在的问题：
    1.客户端 1 创建了 znode 节点/lock，获得了锁。
    2.客户端 1 进入了长时间的 GC pause。（或者网络出现问题、或者 zk 服务检测心跳线程出现问题等等）
    3.客户端 1 连接到 ZooKeeper 的 Session 过期了。znode 节点/lock 被自动删除。
    4.客户端 2 创建了 znode 节点/lock，从而获得了锁。
    5.客户端 1 从 GC pause 中恢复过来，它仍然认为自己持有锁。
    
结论：使用 ZooKeeper 的临时节点实现的分布式锁，它的锁安全期是在客户端取得锁之后到 zk 服务器会话超时的阈值（跨机房部署很容易出现）的时间之间。它无法设置占用分布式锁的时间，何时 zk 服务器会删除锁是不可预知的，所以这种方式它比较适合一些客户端获取到锁之后能够快速处理完毕的场景。
```
```text
ZooKeeper 分布式锁的优点和缺点
优点：
    1.ZooKeeper 分布式锁基于分布式一致性算法实现，能有效的解决分布式问题，不受时钟变迁影响，不可重入问题，使用起来也较为简单；
    2.当锁持有方发生异常的时候，它和 ZooKeeper 之间的 session 无法维护。ZooKeeper 会在 Session 租约到期后，自动删除该 Client 持有的锁，以避免锁长时间无法释放而导致死锁。

缺点：
    ZooKeeper 实现的分布式锁，性能并不太高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。大家知道，ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后 Leader 服务器还需要将数据同步不到所有的 Follower 机器上，这样频繁的网络通信，性能的短板是非常突出的。
```

#### Chubby
```text
Chubby 是 Google 内部使用的分布式锁服务，有点类似于 ZooKeeper，但也存在很多差异。Chubby 对外公开的资料，主要是一篇论文，叫做“The Chubby lock service for loosely-coupled distributed systems”，下载地址如下：

https://research.google.com/archive/chubby.html
```

### 总结
```text
1.基于 ZooKeeper 的分布式锁，适用于高可靠（高可用）而并发量不是太大的场景；
2.基于 Redis 的分布式锁，适用于并发量很大、性能要求很高的、而可靠性问题可以通过其他方案去弥补的场景。
3.基于 MySQL 的分布式锁一般均有单点问题，高并发场景下对数据库的压力比较大；
```