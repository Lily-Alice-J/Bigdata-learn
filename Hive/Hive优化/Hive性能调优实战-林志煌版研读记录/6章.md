##6章

###一、查看SQL的执行计划

    ·查看执行计划的基本信息，即explain；
    ·查看执行计划的扩展信息（抽象语法树），即explain extended； 
    ·查看SQL数据输入依赖的信息（数据来源表分区），即explain dependency；
    ·查看SQL操作相关权限的信息，即explain authorization；
    ·查看SQL的向量化描述信息，即explain vectorization。
    
###二、具体执行计划解读

以某段sql执行计划为例：
    
    STAGE DEPENDENCIES: // 描绘了作业之间的依赖关系
    Stage-1 is a root stage
    Stage-0 depends on stages: Stage-1
    
    STAGE PLANS: // 每个stage的详细信息
    	  Stage: Stage-1
    	    Map Reduce // 表示当前任务执行所用的计算引擎是MapReduce
    	      Map Operator Tree: // 表示当前描述的Map阶段执行的操作信息
    		  // 表示对关键字alias声明的结果集，这里指代dwd_card_apma_cu，进行表扫描操作
              TableScan
                alias: dwd_card_apma_cu
                filterExpr: (((datediff(from_unixtime(unix_timestamp(appdec_day,'yyyymmdd'), 'yyyy-mm-dd'), from_unixtime(unix_timestamp(app_day,'yyyymmdd'), 'yyyy-mm-dd')) > 30) and (not (rtf_state) IN ('D', 'C'))) and (biz_desc = '标准卡')) (type: boolean)
                // 表示对当前阶段的统计信息。例如，当前处理的数据行和数据量，这两个都是预估值
    			Statistics: Num rows: 18660 Data size: 5654095 Basic stats: COMPLETE Column stats: NONE
                // 表示在之前操作（TableScan）的结果集上进行数据的过滤
    			Filter Operator
    			// 表示filter Operator进行过滤时，所用的谓词，即where之后的过滤条件
                  predicate: (((datediff(from_unixtime(unix_timestamp(appdec_day,'yyyymmdd'), 'yyyy-mm-dd'), from_unixtime(unix_timestamp(app_day,'yyyymmdd'), 'yyyy-mm-dd')) > 30) and (not (rtf_state) IN ('D', 'C'))) and (biz_desc = '标准卡')) (type: boolean)
                  // 经过过滤后的数据量，同样也是预估值
    			  Statistics: Num rows: 1555 Data size: 471174 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                  // 表示需要投影的列，即筛选的列
                    expressions: '标准卡' (type: string)
                    outputColumnNames: biz_desc
                    Statistics: Num rows: 1555 Data size: 471174 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                    // 表示分组聚合使用的算法，这里是count(1)
                      aggregations: count(1)
                      // 表示分组的列，在该例子表示的是biz_desc
                      keys: biz_desc (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 1555 Data size: 471174 Basic stats: COMPLETE Column stats: NONE
                      // 表示当前描述的是对之前结果聚会后的输出信息，这里表示Map端聚合后的输出信息
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        // 表示输出是否进行排序，+表示正序，-表示倒序
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 1555 Data size: 471174 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: bigint)
    		  // 表示当前描述的Reduce阶段执行的操作信息
    	      Reduce Operator Tree:
    	        Group By Operator
    	          aggregations: count(VALUE._col0)
    	          keys: KEY._col0 (type: string)
    	          mode: mergepartial
    	          outputColumnNames: _col0, _col1
    	          Statistics: Num rows: 777 Data size: 235435 Basic stats: COMPLETE Column stats: NONE
    	          File Output Operator
    	          // 在File Output Operator中这个关键词表示文件输出的结果是否进行压缩，false表示不进行输出压缩
    	            compressed: false
    	            Statistics: Num rows: 777 Data size: 235435 Basic stats: COMPLETE Column stats: NONE
    	            table:
    	                input format: org.apache.hadoop.mapred.TextInputFormat
    	                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
    	                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
    					
    Stage: Stage-0
    	    Fetch Operator
    	      limit: -1
    	      Processor Tree:
    	        ListSink
    	        
###三、查看SQL数据输入依赖的信息

    3.1 
    explain dependency
    select
    t1.app_no
    from wcl_dwh.apt_card_card_cu t1
    inner join wcl_dwh.apt_card_acct_cu t2
    on t1.acct_no = t2.acct_no 
    and t1.data_date = t2.data_date
    and t1.data_date >= '20191229' and t1.data_date <= '20191231';
    ------------------------------------------------------------
    explain dependency
    select
    t1.app_no
    from wcl_dwh.apt_card_card_cu t1
    inner join wcl_dwh.apt_card_acct_cu t2
    on t1.acct_no = t2.acct_no 
    and t1.data_date = t2.data_date
    where t1.data_date >= '20191229' and t1.data_date <= '20191231';
    ------------------------------------------------------------
    通过上面的输出结果可以看到，其实上述的两个SQL并不等价，在内连接（inner join）中的连接条件中加入非等值的过滤条件后，
    并没有将内连接的左右两个表按照过滤条件进行过滤，内连接在执行时会多读取分区数据
    
    3.2
    explain dependency
    select
    t1.app_no
    from wcl_dwh.apt_card_card_cu t1
    left join wcl_dwh.apt_card_acct_cu t2
    on t1.acct_no = t2.acct_no 
    and t1.data_date = t2.data_date
    and t1.data_date >= '20191229' and t1.data_date <= '20191231';
    ------------------------------------------------------------
    explain dependency
    select
    t1.app_no
    from wcl_dwh.apt_card_card_cu t1
    left join wcl_dwh.apt_card_acct_cu t2
    on t1.acct_no = t2.acct_no 
    and t1.data_date = t2.data_date
    and t2.data_date >= '20191229' and t2.data_date <= '20191231';
    ------------------------------------------------------------
    可以看到，对左外连接在连接条件中加入非等值过滤的条件，如果过滤条件是作用于右表（b表）有起到过滤的效果，则右表只要扫描两个分区即
    可，但是左表（a表）会进行全表扫描。如果过滤条件是针对左表，则完全没有起到过滤的作用，那么两个表将进行全表扫描。这时的情况就如同全外
    连接一样都需要对两个数据进行全表扫描。
    
###四、查看SQL操作涉及的相关权限信息

    explain authorization
    select variance(credit_limit) from wcl_dwh.dwd_card_acct_cu;
    
###五、查看SQL的向量化描述信息

    set hive.vectorized.execution.enabled=true;
    explain vectorization expression