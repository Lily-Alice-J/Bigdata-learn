测试Sqoop是否可以连接到mysql

sqoop  list-databases --connect jdbc:mysql://120.25.122.238:3306/  --username root --password root

用Sqoop将mysql数据导入HDFS

    sqoop import --connect jdbc:mysql://120.25.122.238:3306/sqoop 
    --username root 
    --password root 
    --table student 
    --target-dir /data/sqoop/student -m 1 
  
    sqoop import --connect jdbc:mysql://120.25.122.238:3306/sqoop 
    --username root 
    --password root 
    --query 'select * from user where $CONDITIONS' 
    --split-by id 
    --target-dir /data/sqoop/user2  
    --null-string '' 
    --null-non-string ''

  -m:map并行读取的数量
  
用Sqoop将HDFS数据导入mysql

    sqoop export --connect jdbc:mysql://120.25.122.238:3306/sqoop --username root --password root --table user1  --fields-terminated-by ',' --export-dir /data/sqoop/user

 -Dsqoop.export.records.per.statement=10 
 
指定每10条数据执行一次insert或是

-Dsqoop.export.statements.per.transaction=10 
指定每次事务多少条记录被insert

--update-key id

指定根据那个列进行更新

--columns：指定根据那几个列进行更新，HDFS数据文件叧能包含id,name列用Sqoop将mysql数据导入Hive

    sqoop import 
    
    --connect jdbc:mysql://mysql-server -ip:3306/sqoop  
    
    --username sqoop 
    
    --password sqoop 
    
    --table user_info 
    
    --hive-import
    
参数解析：

--hive-import：指定要导入的hive表


参数解析：

--incremental：指定sqoop增量模式
--check -column:  指定增量的列
--last -value:  指定列值从那一列开始

Sqoop从Hive导入mysql注意事项
导入mysql前mysql必须建立和hive数据格式一致的表。

应确保mysql的数据类型长度大于相对应的hive数据的最大长度。

导入mysql前应确保相应的用户拥有远程登录MySQL的权限。

应该指定null字段的填充符。

如果之前已经导入了部分数据，需要继续增量导入，就必须指定更新的键。--update-key

使用正确的分隔符。

如果你以上几点你都注意了，可能还是会遇到这个错误：

字段对不上或字段类型不一致
Caused by: java.lang.NumberFormatException: For input string:


    sqoop export -D sqoop.export.records.per.statement=10 

    --connect jdbc:mysql://kks1:3306/sougou 
    
    --username hive --password hive 
    
    --table sougou --fields-terminated-by '\t' 
    
    --export-dir "/hivedata/warehouse/sougou.db/sougou/month=6/week=1/day=1"
    
    --null-string '\\N' 
    
    --null-non-string '\\N'
出现这个问题的原因是数据本身的内容含有相应的分隔符，从而导致解析类型错位，进而解析失败。这条记录的源数据如下：

18:47:29    5999958585312108    9    68   
我们使用\t分割，所以sqoop把 什么是OSI参考模型？各层的主要功能是什么？] 当做 int类型解析，进而失败。

Sqoop从Hive导入mysql失败终极解决方案
这个时候，我们可以修改并重新编译sqoop导入mysql解析规则的java文件。每次通过sqoop导入MySql的时，都会在sqoop目录生成一个以MySql表命名的.java文件，然后打成JAR包，给sqoop提交给hadoop 的MR来解析Hive表中的数据。那可以根据报的错误，找到对应的行，改写该文件，编译，重新打包，sqoop可以通过 -jar-file ，–class-name 组合让我们指定运行自己的jar包中的某个class。

编译打包方法如下：
javac -cp ./:/home/sqoop/sqoop-1.4.6.jar:/home/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/home/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar sougou.java
jar -cf sougou.jar sougou.class
运行命令如下：
//hive导入mysql：


    sqoop export -D sqoop.export.records.per.statement=10 

      --connect jdbc:mysql://kks1:3306/sougou 
      
      --username hive 
      
      --password hive 
      
      --table sougou 
      
      --fields-terminated-by '\t' 
      
      --export-dir "/hivedata/warehouse/sougou.db/sougou/month=6/week=1/day=1" 
      
      --null-string '\\N' 
      
      --null-non-string '\\N' 
      
      --jar-file /home/sqoop/sougou.jar 
      
      --class-name sougou;
我们通过修改相关的java文件不仅可以解决各种解析异常，也可以实现自定义多字节列分隔符的功能。
