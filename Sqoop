Sqoop导数至Oracle踩坑记录

业务场景简介：
   客户方为Oracle数仓，需通过Sqoop建立大数据数据集市与传统数仓的传输通道

踩坑记录如下：
<1>Sqoop方面：
1.须在Sqoop /lib目录下存放RDMS驱动包，否则会报无法连接错误
2.在导出数据前可使用sqoop命令进行连接测试
   2.1 sqoop测试连接命令：sqoop list-tables --connect jdbc:oracle:thin:@ip:1521:sid --username ***--password ****
         需注意：Oracle共提供三种JDBC连接方式如下：
                       格式一：jdbc:oracle:thin:@//<host>:<port>/<service_name>  
                       格式二：jdbc:oracle:thin:@<host>:<port>:<SID> 
                       格式三：jdbc:oracle:thin:@<TNSName>
3.需清楚Hive底层存储格式 ORC/TextFile
   3.1 查询命令如下：hadoop fs -cat /user/hive/warehouse/wcl_dwh.db/dws_card_mpur_b_d/000000_0 | head -n 10 | cat -A
4.导数脚本如下：
    4.1 textfile全量版 分区推送export-dir指定相应文件即可；增量导数可见此链接: https://blog.csdn.net/helloxiaozhe/article/details/100176862
        sqoop export \
        --connect jdbc:oracle:thin:@ip:1521:sid\
        --username ***\
        --password ***\
        --export-dir /user/hive/warehouse/wcl_dwh.db/dws_card_mpur_b_d \
        --table DWS_CARD_MPUR_B_D \
        --input-null-string '\\N' \
        --input-null-non-string '\\N' \
        --input-fields-terminated-by '\001' \
        --lines-terminated-by '\n'
    4.2 orc全量版 增量导数可见此链接: https://www.cnblogs.com/lenmom/p/11281536.html
        sqoop export \
        --connect jdbc:oracle:thin:@ip:1521:sid\
        --username ***\
        --password ***\
        --table DWS_CARD_MPUR_B_D \
        --hcatalog-database wcl_dwh \
        --hcatalog-table dws_card_mpur_b_d \
     4.3 批量导出：
        a=0;
        b=1;
        # have_data_table_name是一个文件，里面存为所要迁移的hive表名。
        # cat file_name，for 开始循环表名。
        for table_name in `cat ./hive_data_table_name`
        do
        a=`expr $a + $b`
        echo "表名：$table_name,计数：$a";
        echo  "开始导入数据！"
        ##取出一张表的所有列名，每个列名后加个逗号，然后去掉最后一个逗号，存在col这个变量中。
        ##col=`hive -e "desc wcl_dwh.${table_name}"|sed '1d'|awk '{printf $1","}'|sed 's/,$/\n/g'`
        sqoop export \
        -Dsqoop.export.records.per.statement=10 \
        --connect jdbc:oracle:thin:@172.19.53.145:1521:orl11g \
        --username system \
        --password m89kPoF4 \
        --table ${table_name} \
        --hcatalog-database wcl_dwh \
        --hcatalog-table ${table_name} \
        --batch  
        echo "第${a}张表导入完毕！";
        done
     4.4 sqoop多进程导出
        #!/bin/bash
        declare -i count=0
        declare -i index=0
        declare -i sum=0
        #全量导数
        ARRAY=($(awk -F '\n' '{for(i=1;i<=NF;++i) print $i}'  './tmp/hive_data_table_name'))
        num=${#ARRAY[*]}
        while(($count<=($num/$1)))
        do
        for((i=0;i<$1;++i))
        do
         {
         let sum=$index+$i
         if [ ${sum} -eq $num ];then
           break 2
         fi
        echo "$sum" ##索引
        echo "${ARRAY[$sum]}"
        #tableName=${ARRAY[${sum}]
        sqoop export \
        -Dsqoop.export.records.per.statement=100 \
        --connect jdbc:oracle:thin:@172.19.53.145:1521:orl11g \
        --username system \
        --password m89kPoF4 \
        --table ${ARRAY[${sum}]} \
        --hcatalog-database wcl_dwh \
        --hcatalog-table ${ARRAY[$sum]} \
        --direct \
        --batch
            }&
            done
            wait
            let count++
            let index=$index+$1
            echo "第${count}批导入完成！"
            done

<2> Oracle方面： 
   1. Oracle的服务名(ServiceName)查询
       show parameter service_name;
   2. Oracle的SID查询命令：
       select instance_name from v$instance;
   3. 查看Oracle版本
       select version from v$instance;
   4. 更新分区字段时，可能会遇到如下错误:   ORA-14402: updating partition key column would cause a partition change
     原因解释如下:  更改分区表的分区键值，意味着要删除记录并重新插入一条新的记录，这会引起记录（Record）的移动，记录的Rowid会改变，相关索引需要进行维护。
      alter table table_name enable row movement;
 
<3>优化方面
   1.Oracle并发写入如何加快同步速度？
      添加以下参数  -Dsqoop.export.records.per.statement=10 \ --批量提交，每隔10条提交一次
                            --batch
                            --direct --使用直接导出模式(优化导出速度)
                            --m --提高并行度
   2.如何容错（失败重试）？
      添加以下参数  --staging-table <table-name> \
      由于Sqoop将导出过程切分成了多个，那么就会有可能某个导出任务失败而导致只有部分数据提交到了导出数据库中。当失败job重试时，就有可能会出现数据重复，或者导出数据            冲突等情况发生。这时可以指定一个--staging-table参数来避免这种情况的发生。导出的数据首先会缓存在该表中，最后等job执行成功后会将该表中的数据移动到最终目标表中。
   3.贷前宽表如何实现保留近三天数据-->shell脚本优化
   4.脚本如何更好的支持动态更改表结构-->shell脚本优化
      脚本如下:
            #!/bin/bash
            UN="system"
            PW="m89kPoF4"
            WH="orl11g"
            tableName=$1  #表名
            PT=`date -d "${4} +1 day " +%Y%m%d`  #分区值
            sqlplus ${UN}/${PW}@${WH}<<EOF
            alter table ${tableName}  add  partition part_${4}  values less than('${PT}');
            exit;
            EOF
            echo "${PT}"
            #传递表名,主键,分区字段,分区值
            tableName=$1
            sqoop export \
            --connect jdbc:oracle:thin:@172.19.53.145:1521:orl11g \
            --username system \
            --password m89kPoF4 \
            --table ${tableName^^} \
            --hcatalog-database wcl_dwh \
            --hcatalog-table ${tableName^^}  \
            --hcatalog-partition-keys ${3}  \
            --hcatalog-partition-values ${4} \
            --update-key ${2^^} \
            --update-mode allowinsert

附：
    通过YARN命令查看Job日志
    yarn logs -applicationId application_1592932322109_5986
