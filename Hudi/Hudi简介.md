## Hudi简介

### 一、为什么会有Hudi的出现？

    传统数据解决方案中，常用Hive来构建T+1级别的数据仓库，通过HDFS存储实现海量数据的存储与水平扩容，通过Hive实现元数据的管理以及数据操作的SQL化。虽然能够在海量批处理场景中取得不错的效果，但依然存在如下现状问题：
    
    问题一：不支持事务
    由于传统大数据方案不支持事务，有可能会读到未写完成的数据，造成数据统计错误。为了规避该问题，通常控制读写任务顺序调用，在保证写任务完成后才能启动读任务。但并不是所有读任务都能够被调度系统约束住，在读取时仍存在该问题。
    
    问题二：数据更新效率低
    业务系统库的数据，除流水表类的数据都是新增数据外，还有很多状态类数据表需要更新操作（例如：账户余额表，客户状态表，设备状态表等），而传统大数据方案无法满足增量更新，常采用拉链方式，先进行join操作再进行insert overwrite操作，通过覆盖写的方式完成更新操作，该操作往往需要T+1的批处理模式 ，从而导致端到端数据时延T+1，存在效率低、成本高等问题。
    
    问题三：无法及时应对业务表变化
    上游业务系统对数据schema发生变更后，会导致数据无法入湖，需要数据湖的表schema进行同步调整。从技术实现上采用数据表重建的方式来满足该场景，导致数据湖的数据表的管理与维护方案复杂，实现成本高。另外该种场景通常需要业务部门与数据团队相配合，通过管理流程来实现表结构的同步。
    
    问题四：历史快照表数据冗余
    传统数据湖方案需要对历史的快照表进行存储，采用全量历史存储的方式实现，例如：天级历史快照表，每天都会全量存储全表数据。这样就造成了大量的数据存储冗余，占用大量的存储资源。
    
    问题五：小批量增量数据处理成本高
    传统数据湖为了实现增量ETL，通常将增量数据按照分区的方式进行存储，若为了实现T+0的数据处理，增量数据需要按照小时级或者分钟级的分区粒度。该种实现形式会导致小文件问题，大量分区也会导致元数据服务压力增大。

### 二、什么是Hudi

    摄取与管理处于DFS(HDFS 或云存储)之上的大型分析数据集并为查询访问提供三个逻辑视图。
    读优化视图 - 在纯列式存储上提供出色的查询性能，非常像parquet表。
    增量视图 - 在数据集之上提供一个变更流并提供给下游的作业或ETL任务。
    准实时的表 - 使用基于列存储(例如 Parquet + Avro)和行存储以提供对实时数据的查询
    
### 三、Hudi特色

    1.通过快速，可插入的索引支持Upsert。
    2.通过回滚支持以原子方式发布数据。 
    3.编写器和查询之间的快照隔离。
    4.用于数据恢复的保存点。
    5.使用统计信息管理文件大小，布局。   
    6.行和列数据的异步压缩。
    7.时间轴元数据以跟踪血统。
    8.通过聚类优化数据湖布局。
    9.基于Spark来对HDFS上的数据进行更新、插入、删除等。
    10.可以对HDFS上的parquet格式数据进行插入/更新操作。
    11.通过Savepoint来实现数据恢复。
    12.Hudi是一种针对分析型业务的、扫描优化的数据存储抽象，它能够使HDFS数据集在分钟级的时延内支持变更，也支持下游系统对这个数据集的增量处理。
    13.Hudi数据集通过自定义的InputFormat兼容当前Hadoop生态系统，包括Apache Hive，Apache Parquet，Presto和Apache Spark，使得终端用户可以无缝的对接。
    
### 四、使用场景

    1.近实时摄取：Hudi提供了Hudi表的概念，这些表支持CRUD操作。我们可以基于这个特点，将Mysql Binlog的数据重放至Hudi表，然后基于Hive对Hudi表进行查询分析；
    2.近实时分析：可用使用Presto和SparkSQL对Hudi表进行分析操作
    3.增量处理管道
    4.数据库+实时订阅消费系统
    