##Hudi核心概念一

###一、Hudi数据集的存储
    
    Hudi数据集的组织目录结构与Hive表示非常相似，一份数据集对应这一个根目录。数据集被打散为多个分区，分区字段以文件夹形式存在，该文件夹包含该分区的所有文件。在根目录下，每个分区都有唯一的分区路径。每个分区记录分布于多个文件中。
    每个文件都有惟一的fileId和生成文件的commit所标识。如果发生更新操作时，多个文件共享相同的fileId，但会有不同的commit。
    每条记录由记录的key值进行标识并映射到一个fileId。一条记录的key与fileId之间的映射一旦在第一个版本写入该文件时就是永久确定的。换言之，一个fileId标识的是一组文件，每个文件包含一组特定的记录，不同文件之间的相同记录通过版本号区分。
    
    Hudi Storage由三个不同部分组成：
    
        1.Metadata - 以时间轴（timeline）的形式将数据集上的各项操作元数据维护起来，以支持数据集的瞬态视图，这部分元数据存储于根目录下的元数据目录。
            一共有三种类型的元数据：        
            ·Commits - 一个单独的commit包含对数据集之上一批数据的一次原子写入操作的相关信息。我们用单调递增的时间戳来标识commits，标定的是一次写入操作的开始。        
            ·Cleans - 用于清除数据集中不再被查询所用到的旧版本文件的后台活动。      
            ·Compactions - 用于协调Hudi内部的数据结构差异的后台活动。例如，将更新操作由基于行存的日志文件归集到列存数据上。
            
        2.Index - Hudi维护着一个索引，以支持在记录key存在情况下，将新记录的key快速映射到对应的fileId。索引的实现是插件式的，
            ·Bloom filter - 存储于数据文件页脚。默认选项，不依赖外部系统实现。数据和索引始终保持一致。
            ·Apache HBase - 可高效查找一小批key。在索引标记期间，此选项可能快几秒钟。
            
        3.Data - Hudi以两种不同的存储格式存储所有摄取的数据。这块的设计也是插件式的，用户可选择满足下列条件的任意数据格式：
            ·读优化的列存格式（ROFormat）。缺省值为Apache Parquet
            ·写优化的行存格式（WOFormat）。缺省值为Apache Avro
            
###二、写入方式

    Hudi是一个Spark的第三方库，以Spark Streaming的方式运行数据摄取作业，这些作业一般建议以1~2分钟左右的微批（micro-batch）进行处理。
    
        1.Hudi从相关的分区下的parquet文件中加载BloomFilter索引，并通过传入key值映射到对应的文件来标记是更新还是插入。此处的连接操作可能由于输入数据的大小，分区的分布或者单个分区下的文件数问题导致数据倾斜。
        2.Hudi按分区对insert进行分组，分配一个fileId，然后对相应的日志文件进行append操作，知道文件大小达到HDSF块大小。然后，新的fileId生成，重复上述过程，直到所有的数据都被插入。
        3.Hudi在针对一个fileId进行更新操作时，如果对应的日志文件存在则append，反之，会新建日志文件。
        4.如果数据摄取作业成功，一个commit记录会在Hudi的元数据时间轴中记录，即将inflight文件重命名为commit文件，并将分区和所创建fileId版本的详细信息记录下来。
    
###三、故障恢复

    1.摄取失败可能在日志文件中生成包含部分数据的avro块 - 这个问题通过在commit元数据中存储对应数据块的起始偏移量和日志文件版本来解决。当读取日志文件时，偶尔发生的部分写入的数据块会被跳过，且会从正确的位置开始读取avro文件。
    
    2.Compaction过程失败会生产包含部分数据的parquet文件 - 这个问题在查询阶段被解决，通过commit元数据进行文件版本的过滤。查询阶段只会读取最新的完成的compaction后的文件。这些失败的compaction文件会在下一个compaction周期被回滚。