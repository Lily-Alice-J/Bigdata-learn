## Hudi核心概念三

### 一、写入 Hudi 数据集（DeltaStreamer/Spark/Flink）

    ·UPSERT（插入更新） ：这是默认操作，在该操作中，通过查找索引，首先将输入记录标记为插入或更新。
    在运行启发式方法以确定如何最好地将这些记录放到存储上，如优化文件大小之类后，这些记录最终会被写入。
    对于诸如数据库更改捕获之类的用例，建议该操作，因为输入几乎肯定包含更新。
    
    ·INSERT（插入） ：就使用启发式方法确定文件大小而言，此操作与插入更新（UPSERT）非常相似，但此操作完全跳过了索引查找步骤。
    因此，对于日志重复数据删除等用例（结合下面提到的过滤重复项的选项），它可以比插入更新快得多。
    插入也适用于这种用例，这种情况数据集可以允许重复项，但只需要Hudi的事务写/增量提取/存储管理功能。
    
    ·BULK_INSERT（批插入） ：插入更新和插入操作都将输入记录保存在内存中，以加快存储优化启发式计算的速度（以及其它未提及的方面）。
    所以对Hudi数据集进行初始加载/引导时这两种操作会很低效。批量插入提供与插入相同的语义，但同时实现了基于排序的数据写入算法，
    该算法可以很好地扩展数百TB的初始负载。但是，相比于插入和插入更新能保证文件大小，批插入在调整文件大小上只能尽力而为。
    
    
### 二、查询 Hudi 数据集（Hive/Spark/Presto/Flink）

![image](https://github.com/Tandoy/Bigdata-learn/blob/master/Hudi/images/%E6%9F%A5%E8%AF%A2Hudi%E6%95%B0%E6%8D%AE%E9%9B%86.PNG)


### 三、如何选择存储类型

    写时复制（COW）存储：
    
        ·寻找一种简单的替换现有的parquet表的方法，而无需实时数据。       
        ·当前的工作流是重写整个表/分区以处理更新，而每个分区中实际上只有几个文件发生更改。       
        ·想使操作更为简单（无需压缩等），并且摄取/写入性能仅受parquet文件大小以及受更新影响文件数量限制      
        ·工作流很简单，并且不会突然爆发大量更新或插入到较旧的分区。COW写入时付出了合并成本，因此，这些突然的更改可能会阻塞摄取，并干扰正常摄取延迟目标。
        
    读时合并（MOR）存储：
    
        ·希望数据尽快被摄取并尽可能快地可被查询。       
        ·工作负载可能会突然出现模式的峰值/变化（例如，对上游数据库中较旧事务的批量更新导致对DFS上旧分区的大量更新）。异步压缩（Compaction）有助于缓解由这种情况引起的写放大，而正常的提取则需跟上上游流的变化。