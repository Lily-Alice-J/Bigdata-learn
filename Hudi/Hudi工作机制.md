##Hudi工作机制

    1.Hudi针对HDFS上的数据集提供以下原语
    
        ·插入更新（upsert）
        ·增量消费
    
    Hudi维护在数据集上执行的所有操作的时间轴（timeline），以提供数据集的即时视图。Hudi将数据集组织到与Hive表非常相似的基本路径下的目录结构中。数据集分为多个分区，文件夹包含该分区的文件。每个分区均由相对于基本路径的分区路径唯一标识。
    分区记录会被分配到多个文件。每个文件都有一个唯一的文件ID和生成该文件的提交（commit）。如果有更新，则多个文件共享相同的文件ID，但写入时的提交（commit）不同。
    
    2.存储类型–处理数据的存储方式
    
        ·写时复制
        ·纯列式
        ·创建新版本的文件
        ·读时合并
        ·近实时
    
    3.视图–处理数据的读取方式
    
    4.读取优化视图-输入格式仅选择压缩的列式文件
    
        ·parquet文件查询性能
        ·500 GB的延迟时间约为30分钟
        ·导入现有的Hive表
    
    5.近实时视图
    
        ·混合、格式化数据
        ·约1-5分钟的延迟
        ·提供近实时表
    
    6.增量视图
    
        ·数据集的变更
        ·启用增量拉取
    
    7.Hudi存储层由三个不同的部分组成
    
    元数据–它以时间轴的形式维护了在数据集上执行的所有操作的元数据，该时间轴允许将数据集的即时视图存储在基本路径的元数据目录下。时间轴上的操作类型包括
    
        ·提交（commit），一次提交表示将一批记录原子写入数据集中的过程。单调递增的时间戳，提交表示写操作的开始。
        ·清理（clean），清理数据集中不再被查询中使用的文件的较旧版本。
        ·压缩（compaction），将行式文件转化为列式文件的动作。
        ·索引，将传入的记录键快速映射到文件（如果已存在记录键）。索引实现是可插拔的，Bloom过滤器-由于不依赖任何外部系统，因此它是默认配置，索引和数据始终保持一致。Apache HBase-对少量key更高效。在索引标记过程中可能会节省几秒钟。
        ·数据，Hudi以两种不同的存储格式存储数据。实际使用的格式是可插入的，但要求具有以下特征–读优化的列存储格式（ROFormat），默认值为Apache Parquet；写优化的基于行的存储格式（WOFormat），默认值为Apache Avro。

    