大数据集群环境搭建

四节点创建：

        模板节点配置修改：
        
            1.修改/etc/sysconfig/network-scripts/ifcfg-eth0
                注释掉HWADDR
                改onboot=yes
                BOOTPROTO=static
                根据自己网络环境添加：IPADDR=192.168.XX.XX NETWASK=255.255.255.0 GATEWAY=192.168.XX.2 DNS1=114.114.114.114
            2.关闭防火墙
                chkconfig iptables off （永久关闭）
            3. 进入/etc/seliunx目录修改config文件 SELIUNX=DISABLED
            4. 修改/etc/hosts文件添加ip与主机名映射 192.168.25.11 node01
                                                    192.168.25.12 node02
                                                    192.168.25.13 node03
                                                    192.168.25.14 node04
            5. 在试验环境下删除/etc/udev/rules.d/70-persistent-net.rules文件防止VM再次自动生成HWADDR    
        
        由模板节点克隆出四个节点配置修改：
            1.分别修改四个节点/etc/sysconfig/network-scripts/ifcfg-eth0 IP配置
            2.分别修改/etc/sysconifg/network 修改主机名为node01 node02 node03 node04

Hadoop-HA搭建流程：

 一、伪分布式搭建（在node01机器上搭建）
 
                1.环境配置：ssh、jdk(JAVA_HOME)、时间同步
                    1.1 export JAVA_HOME=/usr/java/jdk1.7.0_67 并在PATH中添加:$JAVA_HOME/bin 然后source /etc/profile
                    1.2 本机ssh免密钥：ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa   
                                       cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
                2.Hadoop环境配置
                    2.1 创建/opt/sxt 用于存放hadoop
                    2.2 export HADOOP_PREFIX=/opt/sxt/hadoop-2.6.5 并在PATH中添加$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin
                3.Hadoop配置文件修改
                    3.1 修改hadoop.env.sh、mapred.env.sh、yarn.env.sh中JAVA_HOME=/usr/java/jdk1.7.0_67
                    3.2 修改core-site.xml
                            <property>
                                <name>fs.defaultFS</name>
                                <value>hdfs://node01:9000</value>
                            </property>
                            <property>
                                <name>hadoop.tmp.dir</name>
                                <value>/var/sxt/hadoop/local</value>
                            </property>
                    3.3 修改hdfs-site.xml
                            <property>
                                <name>dfs.replication</name>
                                <value>2</value>
                            </property>
                            <property>
                                <name>dfs.namenode.secondary.http-address</name>
                                <value>node02:50090</value>
                            </property>
                    3.4 修改slaves为node01
                4.namenode格式化 hdfs namenode -format
                5.start-dfs.sh启动即可

 二、完全分布式搭建（node01、node02、node03、node04）
 
                1.node02、node03、node04节点上安装jdk 并配置JAVA_HOME=/usr/java/jdk1.7.0_67 并在PATH中添加:$JAVA_HOME/bin 然后source /etc/profile
                2.在node01上配置node02、node03、node04的ssh免密
                  scp d_dsa.pub node02:`pwd`/node01.pub
                  scp d_dsa.pub node03:`pwd`/node01.pub
                  scp d_dsa.pub node04:`pwd`/node01.pub
                  然后在node02、node03、node04节点上执行下述命令：
                  cat ~/node1.pub  >> ~/.ssh/authorized_keys
                3.在node02、node03、node04上创建/opt/sxt mkdir /opt/sxt
                4.修改core-site.xml如下
                        <property>
                            <name>fs.defaultFS</name>
                            <value>hdfs://node01:9000</value>
                        </property>
                        <property>
                            <name>hadoop.tmp.dir</name>
                            <value>/var/sxt/hadoop/full</value>
                        </property>
                5.修改hdfs-site.xml如下
                        <property>
                            <name>dfs.replication</name>
                            <value>2</value>
                        </property>
                        <property>
                            <name>dfs.namenode.secondary.http-address</name>
                            <value>node02:50090</value>
                        </property>
                6.修改slaves文件为node02、node03、node04
                7.在node01节点上 cd /opt/sxt
                8.分发至其他三节点 scp -r hadoop-2.6.5  node02:`pwd`
                                   scp -r hadoop-2.6.5  node03:`pwd`
                                   scp -r hadoop-2.6.5  node04:`pwd`
                9.确定伪分别式搭建后的进程已停止
                10.在node01上格式化namenode：hdfs namenode -format
                11.在node01上执行命令：start-dfs.sh
                12.在各节点上验证
                            node01：namenode    node02: datanode    node03: datanode    node04: datanode

 三、Hadoop-HA搭建
 
                HA集群各角色规划如下：
                                                 NN-1    NN-2    DN    ZK    ZKFC    JNN        
                                    node01     *                                         *          *
                                    node02                 *          *       *         *          *
                                    node03                            *        *                    *
                                    node04                            *        *
                
                1.在node02、node03、node04部署Zookeeper
                    1.1 拷贝zoo_sample.cfg 为zoo.cfg
                    1.2 修改zoo.cfg文件 dataDir=/var/sxt/hadoop/zk
                                        server.1=node02:2888:3888
                                        server.2=node03:2888:3888
                                        server.3=node04:2888:3888
                    1.3 mkdir /var/sxt/hadoop/zk 并echo 1 > /var/sxt/hadoop/zk/myid
                    1.4 分发当前目录的zookeeper到node03、node04
                        scp -r zookeeper-3.4.6  node03:`pwd`
                        scp -r zookeeper-3.4.6  node04:`pwd`
                    1.5 node03:mkdir /var/sxt/hadoop/zk 并echo 2 > /var/sxt/hadoop/zk/myid
                        node04:mkdir /var/sxt/hadoop/zk 并echo 3 > /var/sxt/hadoop/zk/myid
                    1.6 使用命令：zkServer.sh start 分别在node02、node03、node04上启动zookeeper
                2.修改hdfs-site.xml如下
                        <property>
                            <name>dfs.nameservices</name>
                            <value>mycluster</value>
                        </property>
                        <property>
                            <name>dfs.ha.namenodes.mycluster</name>
                            <value>nn1,nn2</value>
                        </property>
                        <property>
                            <name>dfs.namenode.rpc-address.mycluster.nn1</name>
                            <value>node01:8020</value>
                        </property>
                        <property>
                            <name>dfs.namenode.rpc-address.mycluster.nn2</name>
                            <value>node02:8020</value>
                        </property>
                        <property>
                            <name>dfs.namenode.http-address.mycluster.nn1</name>
                            <value>node01:50070</value>
                        </property>
                        <property>
                            <name>dfs.namenode.http-address.mycluster.nn2</name>
                            <value>node02:50070</value>
                        </property>
                        <property>
                            <name>dfs.namenode.shared.edits.dir</name>
                            <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
                        </property>
                        <property>
                            <name>dfs.journalnode.edits.dir</name>
                            <value>/var/sxt/hadoop/ha/jn</value>
                        </property>
                        <property>
                        <name>dfs.client.failover.proxy.provider.mycluster</name>
                            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
                            </property>
                        <property>
                            <name>dfs.ha.fencing.methods</name>
                            <value>sshfence</value>
                        </property>
                        <property>
                            <name>dfs.ha.fencing.ssh.private-key-files</name>
                            <value>/root/.ssh/id_dsa</value>
                        </property>
                        <property>
                            <name>dfs.ha.automatic-failover.enabled</name>
                            <value>true</value>
                        </property>
                3.修改core-site.xml
                        <property>
                            <name>fs.defaultFS</name>
                            <value>hdfs://mycluster</value>
                        </property>
                        <property>
                            <name>hadoop.tmp.dir</name>
                            <value>/var/sxt/hadoop/ha</value>
                        </property>
                        <property>
                            <name>ha.zookeeper.quorum</name>
                            <value>node02:2181,node03:2181,node04:2181</value>
                        </property>
                4.配置node02自身ssh免密与node01ssh免密码
                    4.1 ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
                        cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
                    4.2 scp d_dsa.pub node01:`pwd`/node02.pub
                        cat ~/node2.pub  >> ~/.ssh/authorized_keys
                5.在node01当前目录下分发hdfs-site.xml、core-site.xml至node02、node03、node04
                6.在node01、node02、node03上启动journalnode进程
                    hadoop-daemon.sh start journalnode
                7.在node01上执行namenode格式化与namenode的启动
                    hdfs namenode –format
                    hadoop-deamon.sh start namenode
                8.在node02上启动另一个namenode
                    hdfs namenode  -bootstrapStandby
                9.在node01上执行zkfc格式化
                    hdfs zkfc -formatZK
                10.在node01上启动集群所有角色即可
                    start-dfs.sh
                11.按照集群规划使用jps验证所有角色进程是否启动
                
 四、集群关机后启动流程：

                    1.先在node02、node03、node04上启动zookeeper(zkServer.sh start   ||   zkServer.sh status)
                    2.在node01、node02、node03上启动journalnode(hadoop-daemon.sh start journalnode)
                    3.在node01上启动namenode(hadoop-deamon.sh start namenode)
                    4.在node02上启动namenode(hdfs namenode  -bootstrapStandby)
                    5.在node01上执行命令(start-dfs.sh)
